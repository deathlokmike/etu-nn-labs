{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "from torch import nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = tv.transforms.Compose(\n",
    "    [   \n",
    "        tv.transforms.RandomResizedCrop(128),\n",
    "        tv.transforms.ToTensor(),\n",
    "        tv.transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"{os.path.abspath(os.curdir)}/data\"\n",
    "\n",
    "dataset = tv.datasets.ImageFolder(f\"{data_dir}\", data_transforms)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, test_size]\n",
    ")\n",
    "classes = dataset.classes\n",
    "print(\"Classes: \", classes)\n",
    "print(\"The datasest have: \", len(dataset), \" images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def pie_plot(_df):\n",
    "    labels = list(_df.keys())\n",
    "    values = list(_df.values())\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    ax.grid(False)\n",
    "\n",
    "    plt.pie(values, labels=labels, autopct=\"%1.1f%%\")\n",
    "    my_circle = plt.Circle((0, 0), 0.7, color=\"#ffffff\")\n",
    "    plt.title(\"Cоотношение количества объектов разных классов\")\n",
    "    plt.gcf().gca().add_artist(my_circle)\n",
    "\n",
    "train_classes = [label for _, label in train_dataset]\n",
    "pie_plot(dict(Counter(train_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img_np = img.numpy()\n",
    "    plt.imshow(np.transpose(img_np, (1, 2, 0)))\n",
    "\n",
    "\n",
    "for images, labels in train_data_loader:\n",
    "    imshow(tv.utils.make_grid(images))\n",
    "    print(\"Image batch dimensions:\", images.shape)\n",
    "    print(\"Image label dimensions:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гиперпараметры для Conv2d\n",
    "1) **input/output** - кол-во входных и выходных параметров\n",
    "2) **kernel_size** - Размер ядра (окна). Большие ядра могут извлекать более общие признаки, а мелкие - детализированные признаки\n",
    "3) **stride - шаг**. Определяет то, настолько ядро далеко перемещается при каждой операции свертки. Больший шаг уменьшает размер выходной карты признаков. \n",
    "4) **padding** - заполнение. Добавляет рамку вокруг входных данных перед операцие свертки.\n",
    "\n",
    "> Если вход имел размерность w * h, а в слое n сверток размерности kx * ky, то выход будет иметь размерность, n*(w−kx+1)*(h−ky+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCarClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCarClassifier, self).__init__()\n",
    "        # 128x128x3 => 126x126x32\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.activation = nn.ReLU()\n",
    "        self.activation_soft_max = nn.Softmax(dim=1)\n",
    "        self.d1 = nn.Linear(126 * 126 * 32, 256)\n",
    "        self.d2 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 32x1x28x28 => 32x32x26x26\n",
    "        x = self.activation(self.conv1(x))\n",
    "        # flatten => 32 x (32*26*26)\n",
    "        x = self.flatten(x)\n",
    "        # 32 x (32*26*26) => 32x128\n",
    "        x = self.activation(self.d1(x))\n",
    "        # logits => 32x10\n",
    "        x = self.activation_soft_max(self.d2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedCarClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedCarClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(126 * 126 * 64, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activation(self.conv1(x)))\n",
    "        x = self.pool(self.activation(self.conv2(x)))\n",
    "        x = self.pool(self.activation(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гиперпараметры для Pool2D\n",
    "1) **Тип операции объединения** - не является параметром. Можно изменить, используя импортирование различных функций (максимальное, среднее). \n",
    "\n",
    "    Максимальное объединение часто используется для выделения ключевых признаков, тогда как среднее объединение может быть полезным для создания более устойчивых признаков.\n",
    "2) **kernel_size** - размер ядра (окна объединение). \n",
    "    \n",
    "    Большие размеры окон объединения уменьшат размер карт признаков более агрессивно и могут привести к уменьшению пространственной разрешающей способности сети. Меньшие размеры окон сохраняют более детализированную информацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten - сворачивает n-мерный тензор в одномерный\n",
    "\n",
    "> Когда применяется сверточный слой к изображению, выходом является трехмерный массив (высота, ширина, количество каналов). Перед передачей данных в полносвязный слой, который ожидает одномерный вектор, требуется операция \"flatten\" для преобразования трехмерного массива в одномерный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnotherCarClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(AnotherCarClassifier, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activation(self.conv1(x)))\n",
    "        x = self.pool(self.activation(self.conv2(x)))\n",
    "        x = self.pool(self.activation(self.conv3(x)))\n",
    "        x = self.drop(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def losses_plot(losses: list[float]):\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Эпоха\")\n",
    "    plt.ylabel(\"Потери\")\n",
    "    plt.title(\"Потери в обучении на протяжении эпох\")\n",
    "    plt.show()\n",
    "\n",
    "def accuracy_plot(accuracy: list[float]):\n",
    "    plt.plot(accuracy)\n",
    "    plt.xlabel(\"Эпоха\")\n",
    "    plt.ylabel(\"Точность\")\n",
    "    plt.title(\"Точность обучения на протяжении эпох\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.2f'\n",
    "        print(\"Нормализованная матрица ошибок\")\n",
    "    else:\n",
    "        fmt = 'd'\n",
    "        print('Матрица ошибок без нормализации')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    plt.ylabel('Истинные метки')\n",
    "    plt.xlabel('Предсказанные метки')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > 0.3 else \"black\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(logit, target, batch_size):\n",
    "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects / batch_size\n",
    "    return accuracy.item()\n",
    "\n",
    "def get_model_accuracy(_net):\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true_list = []\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_data_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = _net(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            y_true_list.extend(labels.numpy())\n",
    "            y_pred_list.extend(predictions.numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            # собираем правильные прогнозы для каждого класса\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "    \n",
    "    print(f\"Точность сети: {100 * correct // total}%\")\n",
    "\n",
    "    # Выводим точность на каждом классе\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(f'Точность для класса: {classname} is {accuracy:.1f}%')\n",
    "\n",
    "    y_true = np.array(y_true_list)\n",
    "    y_pred = np.array(y_pred_list)\n",
    "\n",
    "    plot_confusion_matrix(y_true, y_pred, classes, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(_criterion, _optimizer, _net, _epoch_num):\n",
    "    epoch_loss: list[float] = []\n",
    "    epoch_accuracy: list[float] = []\n",
    "\n",
    "    for epoch in range(_epoch_num):\n",
    "        running_loss = 0.0\n",
    "        accuracy = 0.0\n",
    "        for images, labels in train_data_loader:\n",
    "\n",
    "            inputs, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            _optimizer.zero_grad()\n",
    "\n",
    "            outputs = _net(inputs)\n",
    "            loss = _criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            _optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # accuracy += get_accuracy(outputs, labels, BATCH_SIZE)\n",
    "        epoch_loss_temp = running_loss / len(train_data_loader)\n",
    "        epoch_accuracy_temp = accuracy / len(train_data_loader)\n",
    "\n",
    "        epoch_loss.append(epoch_loss_temp)\n",
    "        epoch_accuracy.append(epoch_accuracy_temp)\n",
    "        print(\n",
    "            \"Epoch: %d | Loss: %.4f | Train Accuracy: %.2f \"\n",
    "            % (epoch, epoch_loss_temp, epoch_accuracy_temp)\n",
    "        )\n",
    "\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "simple_model = SimpleCarClassifier().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(simple_model.parameters(), lr=0.001)\n",
    "simple_losses, simple_accuracy = train_model(criterion, optimizer, simple_model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_plot(simple_losses)\n",
    "accuracy_plot(simple_accuracy)\n",
    "get_model_accuracy(simple_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_resnet = tv.models.resnet18(pretrained=True)\n",
    "\n",
    "# Заморозка параметров нижних слоев\n",
    "for param in pretrained_resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Изменение архитектуры верхних слоев\n",
    "pretrained_resnet.fc = nn.Sequential(\n",
    "    nn.Linear(pretrained_resnet.fc.in_features, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 2),  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preptrained_model = pretrained_resnet\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(preptrained_model.parameters(), lr=0.001)\n",
    "pretrained_losses, pretrained_accuracy = train_model(criterion, optimizer, preptrained_model, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_plot(pretrained_losses)\n",
    "accuracy_plot(pretrained_accuracy)\n",
    "get_model_accuracy(preptrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_model = ModifiedCarClassifier()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(modified_model.parameters(), lr=0.001)\n",
    "modified_losses, modified_accuracy = train_model(criterion, optimizer, modified_model, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_plot(modified_losses)\n",
    "accuracy_plot(modified_accuracy)\n",
    "get_model_accuracy(modified_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_model = AnotherCarClassifier()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(another_model.parameters(), lr=0.001)\n",
    "another_losses, another_accuracy = train_model(criterion, optimizer, another_model, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_plot(another_losses)\n",
    "accuracy_plot(another_accuracy)\n",
    "get_model_accuracy(another_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
